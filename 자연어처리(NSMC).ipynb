{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어처리.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMK4Q1uGblT2claAbDZNLr5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoreaUnivseokjinmoon/seokjinmoonNLU/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC(NSMC).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JjbUihKuS6_"
      },
      "source": [
        "1. BERT 모델을 다운로드\n",
        "BERT-Base, Multilingual Cased: 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNcb2MaQ8RDG",
        "outputId": "d67cd34d-b35a-4eb6-9c32-f5ffa4f7c992"
      },
      "source": [
        "# wget을 활용해서 bert 모델 다운로드 가능\r\n",
        "import os\r\n",
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\r\n",
        "\r\n",
        "if \"bert\" not in os.listdir():\r\n",
        "  os.makedirs(\"bert\")\r\n",
        "else:\r\n",
        "  pass\r\n",
        "\r\n",
        "import zipfile\r\n",
        "import shutil\r\n",
        "         \r\n",
        "bert_zip = zipfile.ZipFile('multi_cased_L-12_H-768_A-12.zip')\r\n",
        "bert_zip.extractall('bert')\r\n",
        " \r\n",
        "bert_zip.close()\r\n",
        "\r\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\r\n",
        "    for item in os.listdir(src):\r\n",
        "        s = os.path.join(src, item)\r\n",
        "        d = os.path.join(dst, item)\r\n",
        "        if os.path.isdir(s):\r\n",
        "            shutil.copytree(s, d, symlinks, ignore)\r\n",
        "        else:\r\n",
        "            shutil.copy2(s, d)\r\n",
        "\r\n",
        "copytree(\"bert/multi_cased_L-12_H-768_A-12\", \"bert\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-24 12:45:53--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.31.128, 172.217.7.208, 172.217.9.208, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.31.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip.3’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M   198MB/s    in 3.2s    \n",
            "\n",
            "2020-12-24 12:45:56 (198 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip.3’ saved [662903077/662903077]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSXwnOYVwQsc"
      },
      "source": [
        "2. 구글 Drive와 Google Colab 연동"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpAYEreWuPkC"
      },
      "source": [
        "import os\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive/')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG7L3g7awbCz"
      },
      "source": [
        "#os.listdir('gdrive/My Drive/Korea_Univ_NLP')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nErPk_9mw_v_"
      },
      "source": [
        "3. BERT 모델이 있는 Path를 변수로 지정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n5sxQs0Qwzla",
        "outputId": "1c95cbea-5c5a-44d5-87ea-f345e45c4163"
      },
      "source": [
        "path = \"gdrive/My Drive/Korea_Univ_NLP\"\n",
        "path"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'gdrive/My Drive/Korea_Univ_NLP'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA_-n25Ixls_"
      },
      "source": [
        "4. 프로젝트 시 필요한 모듈 Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXSTu2Q6xHM-"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import re\n",
        "import pickle\n",
        "\n",
        "import keras as keras\n",
        "from keras.models import load_model\n",
        "from keras import backend as K\n",
        "from keras import Input, Model\n",
        "from keras import optimizers\n",
        "\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import shutil"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV92-9e_yAKP"
      },
      "source": [
        "5. BERT 모델 활용을 위한 keras-bert & \n",
        "Adam optimizer의 수정판인 keras-radam 모듈을 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RUeAn8ExpCo",
        "outputId": "b0fbbb6b-e736-475f-f455-fc0d92807369"
      },
      "source": [
        "!pip install keras-bert"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-bert in /usr/local/lib/python3.6/dist-packages (0.86.0)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.4.3)\n",
            "Requirement already satisfied: keras-transformer>=0.38.0 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (0.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.19.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Requirement already satisfied: keras-embed-sim>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert) (0.8.0)\n",
            "Requirement already satisfied: keras-layer-normalization>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert) (0.14.0)\n",
            "Requirement already satisfied: keras-pos-embd>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert) (0.11.0)\n",
            "Requirement already satisfied: keras-position-wise-feed-forward>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert) (0.6.0)\n",
            "Requirement already satisfied: keras-multi-head>=0.27.0 in /usr/local/lib/python3.6/dist-packages (from keras-transformer>=0.38.0->keras-bert) (0.27.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Requirement already satisfied: keras-self-attention==0.46.0 in /usr/local/lib/python3.6/dist-packages (from keras-multi-head>=0.27.0->keras-transformer>=0.38.0->keras-bert) (0.46.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQSA60VpylFP"
      },
      "source": [
        "6. keras-bert 라이브러리에서 BERT 모형 활용에 필요한 모듈들을 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GrdTF7zyKk8"
      },
      "source": [
        "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
        "from keras_bert import Tokenizer\n",
        "from keras_bert import AdamWarmup, calc_train_steps\n",
        "\n",
        "#from keras_radam import RAdam"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLgsTnZuyrZ9",
        "outputId": "dfd4daa2-26cb-433f-979a-02ea624ad6a1"
      },
      "source": [
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nsmc'...\n",
            "remote: Enumerating objects: 14763, done.\u001b[K\n",
            "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
            "Receiving objects: 100% (14763/14763), 56.19 MiB | 20.90 MiB/s, done.\n",
            "Resolving deltas: 100% (1749/1749), done.\n",
            "Checking out files: 100% (14737/14737), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lx-STuGv2du8",
        "outputId": "e95bc753-7d6d-441a-d7aa-229667a1ca8e"
      },
      "source": [
        "os.listdir('nsmc')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['raw',\n",
              " 'ratings_test.txt',\n",
              " 'ratings.txt',\n",
              " 'README.md',\n",
              " 'code',\n",
              " '.git',\n",
              " 'synopses.json',\n",
              " 'ratings_train.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yB-8LhdwnGYc",
        "outputId": "0660a4a0-582e-47aa-d6a5-b5d728412e55"
      },
      "source": [
        "!git clone https://github.com/KoreaUnivseokjinmoon/seokjinmoonNLU.git"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seokjinmoonNLU'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 25 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyJ3kYHCnLb1",
        "outputId": "75b62513-42ba-4a8b-b50b-afb47d5f847b"
      },
      "source": [
        "os.listdir('seokjinmoonNLU')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['friends_en', 'README.md', 'testSet', '.git']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpXzu1_m5jZw"
      },
      "source": [
        "BERT 모델을 훈련시킬 데이터를 Train Data와 Test Data로 나눈다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veBVWuoh40lv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed98c414-16fb-4fdf-e1aa-daea2ced4f4b"
      },
      "source": [
        "train = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n",
        "test = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")\n",
        "\n",
        "temp = pd.read_csv(\"seokjinmoonNLU/testSet/\"+\"ko_data.csv\", encoding='CP949')\n",
        "\n",
        "#train = pd.concat([train, test])\n",
        "\n",
        "\n",
        "temp.columns = ['id', 'document']\n",
        "test = temp\n",
        "\n",
        "train.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150000, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "-FYtzCIs5ox5",
        "outputId": "07acd513-7c0b-4f1d-b7d6-0b31542406a7"
      },
      "source": [
        "train[149998:150010]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>2376369</td>\n",
              "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>9619869</td>\n",
              "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             id                     document  label\n",
              "149998  2376369  청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
              "149999  9619869     한국 영화 최초로 수간하는 내용이 담긴 영화      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "m8WX6VQ2n_xT",
        "outputId": "42247e96-693a-4ceb-9d5d-9b8e5d87086a"
      },
      "source": [
        "test[50:70]"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>50</td>\n",
              "      <td>너무 야하다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>51</td>\n",
              "      <td>더 이상 숨어서는 안 된다.그냥 지나갈 뿐...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>52</td>\n",
              "      <td>로맨스이긴 하지만 내용의 흐림이 어쩐지 어색한 것</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>53</td>\n",
              "      <td>글쎄요, 너무 평범해서 시간낭비하고 싶어요.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>54</td>\n",
              "      <td>허버 굳이 미야이즈미 죽이지마 -- 죽일 때가 기억에 남지 마</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>55</td>\n",
              "      <td>10점이 아깝지 않다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>56</td>\n",
              "      <td>인권이라는 주제를 잘 표현한 영화</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>57</td>\n",
              "      <td>천재적인 아이들이 마음에 끌렸다고 생각합니다.~^-^!!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>58</td>\n",
              "      <td>이거는 거의 플루시맨을 보는 것 같은... 무슨 영화가 뭐야, 그래픽도.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>59</td>\n",
              "      <td>가족호러에 에디머피가 나오면 재미도 없고 호러도 아니라는 결론이 나온다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>60</td>\n",
              "      <td>희망을 버리지 않고, 결국은 쟁취한다. 그러나 암울하다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>61</td>\n",
              "      <td>ㅋㅋㅋ 혹평이라니 ㅋㅋㅋ그때 당시 미국 개봉 때 유행했는데...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>62</td>\n",
              "      <td>요즘도 반히포크라테스가 시행되고 있는지 궁금합니다.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>63</td>\n",
              "      <td>이것은 왜 19살입니까? 공포때문에??</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>64</td>\n",
              "      <td>비디오로 보는것도 시간과 돈이 아깝다...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>65</td>\n",
              "      <td>금년 최악의 영화다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>66</td>\n",
              "      <td>캐스팅만 화려해.정말... 다른게 남는게 없어</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>67</td>\n",
              "      <td>음악을 아는 사람에게는 1000점을 줘도 모자란 영화.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>68</td>\n",
              "      <td>정말 못 참겠네. -_-+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>69</td>\n",
              "      <td>주인공들의 비극적인 종말과 그들의 관계... 도저히 걸을 수 없는 현실</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id                                  document\n",
              "50  50                                    너무 야하다\n",
              "51  51                더 이상 숨어서는 안 된다.그냥 지나갈 뿐...\n",
              "52  52               로맨스이긴 하지만 내용의 흐림이 어쩐지 어색한 것\n",
              "53  53                  글쎄요, 너무 평범해서 시간낭비하고 싶어요.\n",
              "54  54        허버 굳이 미야이즈미 죽이지마 -- 죽일 때가 기억에 남지 마\n",
              "55  55                               10점이 아깝지 않다\n",
              "56  56                        인권이라는 주제를 잘 표현한 영화\n",
              "57  57           천재적인 아이들이 마음에 끌렸다고 생각합니다.~^-^!!\n",
              "58  58  이거는 거의 플루시맨을 보는 것 같은... 무슨 영화가 뭐야, 그래픽도.\n",
              "59  59   가족호러에 에디머피가 나오면 재미도 없고 호러도 아니라는 결론이 나온다\n",
              "60  60            희망을 버리지 않고, 결국은 쟁취한다. 그러나 암울하다\n",
              "61  61       ㅋㅋㅋ 혹평이라니 ㅋㅋㅋ그때 당시 미국 개봉 때 유행했는데...\n",
              "62  62              요즘도 반히포크라테스가 시행되고 있는지 궁금합니다.\n",
              "63  63                     이것은 왜 19살입니까? 공포때문에??\n",
              "64  64                   비디오로 보는것도 시간과 돈이 아깝다...\n",
              "65  65                                금년 최악의 영화다\n",
              "66  66                 캐스팅만 화려해.정말... 다른게 남는게 없어\n",
              "67  67            음악을 아는 사람에게는 1000점을 줘도 모자란 영화.\n",
              "68  68                            정말 못 참겠네. -_-+\n",
              "69  69   주인공들의 비극적인 종말과 그들의 관계... 도저히 걸을 수 없는 현실"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKW1OQFd6krz"
      },
      "source": [
        "BERT 훈련을 위한 사전 설정\n",
        "SEQ_LEN : 문장의 최대 길이\n",
        "BATCH_SIZE : 메모리 초과 같은 문제를 방지하기 위해 작은 수인 16으로 정했습니다.\n",
        "EPOCHS : 총 훈련 에포크 수\n",
        "LR : 학습률, Learning rate\n",
        "pretrained_path는 bert 사전학습 모형이 있는 폴더를 의미합니다.\n",
        "그리고 우리가 분석할 문장이 들어있는 칼럼의 제목인 document와 긍정인지 부정인지 알려주는 칼럼을 label로 정해줍니다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJIosvk35sAj"
      },
      "source": [
        "SEQ_LEN = 128\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS=2\n",
        "LR=1e-5\n",
        " \n",
        "pretrained_path =\"bert\"\n",
        "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
        "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
        "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
        " \n",
        "DATA_COLUMN = \"document\"\n",
        "LABEL_COLUMN = \"label\""
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3TgFRxE8uyR"
      },
      "source": [
        "token_dict = {}\n",
        "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
        "    for line in reader:\n",
        "        token = line.strip()\n",
        "        if \"_\" in token:\n",
        "          token = token.replace(\"_\",\"\")\n",
        "          token = \"##\" + token\n",
        "        token_dict[token] = len(token_dict)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxDi2x4q_xTa"
      },
      "source": [
        "class inherit_Tokenizer(Tokenizer):\n",
        "  def _tokenize(self, text):\n",
        "        if not self._cased:\n",
        "            text = text\n",
        "            \n",
        "            text = text.lower()\n",
        "        spaced = ''\n",
        "        for ch in text:\n",
        "            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n",
        "                spaced += ' ' + ch + ' '\n",
        "            elif self._is_space(ch):\n",
        "                spaced += ' '\n",
        "            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
        "                continue\n",
        "            else:\n",
        "                spaced += ch\n",
        "        tokens = []\n",
        "        for word in spaced.strip().split():\n",
        "            tokens += self._word_piece_tokenize(word)\n",
        "        return tokens"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u22A0RxVF07S"
      },
      "source": [
        "tokenizer = inherit_Tokenizer(token_dict)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4rvU0BNF4A1"
      },
      "source": [
        "def convert_data(data_df):\n",
        "    global tokenizer\n",
        "    indices, targets = [], []\n",
        "    for i in tqdm(range(len(data_df))):\n",
        "        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n",
        "        indices.append(ids)\n",
        "        targets.append(data_df[LABEL_COLUMN][i])\n",
        "    items = list(zip(indices, targets))\n",
        "    \n",
        "    indices, targets = zip(*items)\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)], np.array(targets)\n",
        "\n",
        "def load_data(pandas_dataframe):\n",
        "    data_df = pandas_dataframe\n",
        "    \n",
        "    \n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
        "\n",
        "\n",
        "    data_x, data_y = convert_data(data_df)\n",
        "\n",
        "    return data_x, data_y"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwzexxi_8fWJ",
        "outputId": "2c33d7a2-a14d-460c-8259-19c1802f392b"
      },
      "source": [
        "train_x, train_y = load_data(train)\n",
        "#test_x, test_y = load_data(test)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 150000/150000 [00:19<00:00, 7698.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puY2W0jk8gdE"
      },
      "source": [
        "def sentence_convert_data(data): \n",
        "    global tokenizer\n",
        "    indices = []\n",
        "    for i in tqdm(range(len(data))):\n",
        "        print(tokenizer.tokenize(data[i]))\n",
        "        ids, segments = tokenizer.encode(data[i], max_len=SEQ_LEN)\n",
        "        indices.append(ids)\n",
        "        \n",
        "    items = indices\n",
        "    indices = np.array(indices)\n",
        "    return [indices, np.zeros_like(indices)]\n",
        "\n",
        "def sentence_load_data(sentences):#sentence는 List로 받는다\n",
        "           \n",
        "    data_x = sentence_convert_data(sentences)\n",
        "\n",
        "    return data_x"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1lJhJPL-r5U"
      },
      "source": [
        "layer_num = 12\n",
        "model = load_trained_model_from_checkpoint(\n",
        "    config_path,\n",
        "    checkpoint_path,\n",
        "    training=True,\n",
        "    trainable=True,\n",
        "    seq_len=SEQ_LEN,)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSw-lpWH_GMa",
        "outputId": "26d7837c-f5ba-45e4-89bd-b218d736447b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (TokenEmbedding [(None, 128, 768), ( 91812096    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, 128, 768)     1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, 128, 768)     0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, 128, 768)     98304       Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, 128, 768)     0           Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, 128, 768)     1536        Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    2362368     Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-1-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     0           Embedding-Norm[0][0]             \n",
            "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-1-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-1-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-1-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-2-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-2-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-2-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-2-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-3-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-3-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-3-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-3-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-4-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-4-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-4-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-4-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-5-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-5-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-5-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-5-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-6-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-6-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-6-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-6-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-7-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-7-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-7-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-7-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-8-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-8-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-8-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    2362368     Encoder-8-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, None, 768)    0           Encoder-9-MultiHeadSelfAttention[\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-MultiHeadSelfAttentio (None, 128, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward (FeedForw (None, 128, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Dropout ( (None, 128, 768)     0           Encoder-9-FeedForward[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Add (Add) (None, 128, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
            "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-9-FeedForward-Norm (Lay (None, 128, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-9-FeedForward-Norm[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
            "                                                                 Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Dropout  (None, 128, 768)     0           Encoder-10-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Add (Add (None, 128, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
            "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-10-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-10-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Dropout  (None, 128, 768)     0           Encoder-11-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Add (Add (None, 128, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
            "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-11-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    2362368     Encoder-11-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, None, 768)    0           Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
            "                                                                 Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-MultiHeadSelfAttenti (None, 128, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward (FeedFor (None, 128, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Dropout  (None, 128, 768)     0           Encoder-12-FeedForward[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Add (Add (None, 128, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
            "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
            "__________________________________________________________________________________________________\n",
            "Encoder-12-FeedForward-Norm (La (None, 128, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "MLM-Dense (Dense)               (None, 128, 768)     590592      Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Norm (LayerNormalization)   (None, 128, 768)     1536        MLM-Dense[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "MLM-Sim (EmbeddingSimilarity)   (None, 128, 119547)  119547      MLM-Norm[0][0]                   \n",
            "                                                                 Embedding-Token[0][1]            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Masked (InputLayer)       [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "MLM (Masked)                    (None, 128, 119547)  0           MLM-Sim[0][0]                    \n",
            "                                                                 Input-Masked[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "NSP (Dense)                     (None, 2)            1538        NSP-Dense[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 178,271,741\n",
            "Trainable params: 178,271,741\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRJMncXU_T46"
      },
      "source": [
        "def get_bert_finetuning_model(model):\n",
        "  inputs = model.inputs[:2]\n",
        "  dense = model.layers[-3].output\n",
        " \n",
        " \n",
        "  outputs = keras.layers.Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
        "                              name = 'real_output')(dense)\n",
        " \n",
        " \n",
        " \n",
        "  bert_model = keras.models.Model(inputs, outputs)\n",
        "  bert_model.compile(\n",
        "      optimizer=tf.optimizers.Adam(learning_rate=0.00001, beta_1=0.9),\n",
        "      loss='binary_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "  \n",
        "  return bert_model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL9ibUFq_zGg",
        "outputId": "cf16de29-9eba-46d7-f937-f95a175927b5"
      },
      "source": [
        "bert_model = get_bert_finetuning_model(model)\n",
        " \n",
        "history = bert_model.fit(train_x, train_y, epochs=3, batch_size=32, verbose = 1, validation_split=0.25, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "  84/3516 [..............................] - ETA: 1:27:37 - loss: 0.6751 - accuracy: 0.5701"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GnhI-EaSZZA"
      },
      "source": [
        " #bert_model.save_weights(path+\"/bertv2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vlS3ohKSdhH"
      },
      "source": [
        "bert_model = get_bert_finetuning_model(model)\n",
        "#bert_model.load_weights(path+\"/bertv2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSmkDnOCdyjc"
      },
      "source": [
        "def predict_convert_data(data_df):\r\n",
        "    global tokenizer\r\n",
        "    indices = []\r\n",
        "    for i in tqdm(range(len(data_df))):\r\n",
        "        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\r\n",
        "        indices.append(ids)\r\n",
        "        \r\n",
        "    items = indices\r\n",
        "    \r\n",
        "    \r\n",
        "    indices = np.array(indices)\r\n",
        "    return [indices, np.zeros_like(indices)]\r\n",
        "\r\n",
        "def predict_load_data(x): #Pandas Dataframe을 인풋으로 받는다\r\n",
        "    data_df = x\r\n",
        "    \r\n",
        "    \r\n",
        "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\r\n",
        "\r\n",
        "\r\n",
        "    data_x = predict_convert_data(data_df)\r\n",
        "\r\n",
        "    return data_x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guHeRiDReFM5"
      },
      "source": [
        "test_set = predict_load_data(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OjAVIw1eIga"
      },
      "source": [
        "preds = ( bert_model.predict(test_set) > 0.5 ).astype(\"int32\")\r\n",
        "#preds = np.argmax(bert_model.predicttest_set)\r\n",
        "#preds = bert_model.predict_classes(test_set, verbose = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcdMKvbqeLK1"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmboXoNY_RqU"
      },
      "source": [
        "testCSV = test.copy()\r\n",
        "del testCSV['document']\r\n",
        "result = pd.DataFrame(preds)\r\n",
        "result\r\n",
        "\r\n",
        "finCSV = pd.concat([testCSV, result], axis=1)\r\n",
        "finCSV.columns = ['id', 'Predicted']\r\n",
        "finCSV.to_csv(\"sample.csv\", mode='w', index = False)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZcApm0aCRvV"
      },
      "source": [
        "# 새 섹션"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmVJ8mpISYst"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJFxxxQvACld"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}