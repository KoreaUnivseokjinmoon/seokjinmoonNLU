{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "자연어처리_en.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPbKddF3w4sicGdN0U38or4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KoreaUnivseokjinmoon/seokjinmoonNLU/blob/main/%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_en(Friends).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNdw0vg8-U2M"
      },
      "source": [
        "1. Pretrained Model 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sR3CnUEt-Z2P",
        "outputId": "93ff6053-cadb-472f-e5a2-60813a709a9e"
      },
      "source": [
        "# wget을 활용해서 bert 모델 다운로드 가능\r\n",
        "import os\r\n",
        "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\r\n",
        "\r\n",
        "if \"bert\" not in os.listdir():\r\n",
        "  os.makedirs(\"bert\")\r\n",
        "else:\r\n",
        "  pass\r\n",
        "\r\n",
        "import zipfile\r\n",
        "import shutil\r\n",
        "         \r\n",
        "bert_zip = zipfile.ZipFile('multi_cased_L-12_H-768_A-12.zip')\r\n",
        "bert_zip.extractall('bert')\r\n",
        " \r\n",
        "bert_zip.close()\r\n",
        "\r\n",
        "def copytree(src, dst, symlinks=False, ignore=None):\r\n",
        "    for item in os.listdir(src):\r\n",
        "        s = os.path.join(src, item)\r\n",
        "        d = os.path.join(dst, item)\r\n",
        "        if os.path.isdir(s):\r\n",
        "            shutil.copytree(s, d, symlinks, ignore)\r\n",
        "        else:\r\n",
        "            shutil.copy2(s, d)\r\n",
        "\r\n",
        "copytree(\"bert/multi_cased_L-12_H-768_A-12\", \"bert\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-21 12:03:19--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.193.128, 172.217.204.128, 172.217.203.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.193.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M   177MB/s    in 3.7s    \n",
            "\n",
            "2020-12-21 12:03:23 (173 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HatROLtoCf2U"
      },
      "source": [
        "2. 필요한 Module Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRJgx-wR-aRr"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np  \r\n",
        "import re\r\n",
        "import pickle\r\n",
        "\r\n",
        "import keras as keras\r\n",
        "from keras.models import load_model\r\n",
        "from keras import backend as K\r\n",
        "from keras import Input, Model\r\n",
        "from keras import optimizers\r\n",
        "\r\n",
        "import codecs\r\n",
        "from tqdm import tqdm\r\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RpNACgXEVOo"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7PFMaE-Cl9u"
      },
      "source": [
        "3. Keras-bert Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeX920fICqy0",
        "outputId": "f20a1a13-abe4-4f7a-e34f-f654396a93a5"
      },
      "source": [
        "!pip install keras-bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.19.4)\n",
            "Requirement already satisfied: Keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.4.3)\n",
            "Collecting keras-transformer>=0.38.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n",
            "Collecting keras-pos-embd>=0.11.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.27.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.14.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n",
            "Collecting keras-self-attention==0.46.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=c4485ccc3540942bf5f1390b65c9c531c40d848e906f0bfb0859e49c5151aec5\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12944 sha256=dae571776db86a0ecd12479072ac9e60825bd56ceb4a291e7eae5378352bfabd\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=cc8e3a802c302922b3f4d30b0f81178848a040d7175447a3b09b74aa1810267d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=0c53b9d1aef27009d833eaa2c7ea2040512c525defaabb7df46073afa7a37755\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5267 sha256=903da4a8f618697c97c8c1b421f870f7f5e19bf288912f5f39fcc76ab4ba82b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5625 sha256=f74a8d26f2f192b443a7be523ed653e2cb955e55c62a995b4c817ee90859e50a\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=8ca476d214a9615ecb74456ef52b5793bf6a3d98ec52a4037ace957604e91f56\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=856add4d8205f4ba66a0669fea75a4f6d70579997d0c82926d8dc924a1935496\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8Hz41G1C1Dj"
      },
      "source": [
        "Keras-bert 라이브러리에서 필요한 Module Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spe9Chd7Cyc0"
      },
      "source": [
        "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\r\n",
        "from keras_bert import Tokenizer\r\n",
        "from keras_bert import AdamWarmup, calc_train_steps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3hVGfkGC7rm"
      },
      "source": [
        "4. Train, Test 데이터 다운"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBTzycpPC_Uq",
        "outputId": "56f8b8ec-1801-4f0d-f5a2-d443b775c57a"
      },
      "source": [
        "!git clone https://github.com/KoreaUnivseokjinmoon/seokjinmoonNLU.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'seokjinmoonNLU'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 25 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrREipUKDa4W",
        "outputId": "fbfa72b3-b9a1-400b-c366-e572ec15f81f"
      },
      "source": [
        "os.listdir('seokjinmoonNLU')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['testSet', '.git', 'friends_en', 'README.md']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "X5I4lNXxEMWw",
        "outputId": "6deab22f-658a-4a5a-d6d3-1dbaa0aca37d"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "obj = {}\r\n",
        "obj2 = {}\r\n",
        "\r\n",
        "train_path = \"seokjinmoonNLU/friends_en/\"+\"friends_train.json\"\r\n",
        "dev_path = \"seokjinmoonNLU/friends_en/\"+\"friends_dev.json\"\r\n",
        "test_path = \"seokjinmoonNLU/friends_en/\"+\"friends_test.json\"\r\n",
        "\r\n",
        "\r\n",
        "#train = json.loads(train_path)\r\n",
        "\r\n",
        "#with open(train_path) as jsonfile:\r\n",
        "#    obj = json.load(jsonfile)\r\n",
        "\r\n",
        "\r\n",
        "#with open(\"./seokjinmoonNLU/friends_en/\"+\"friends_test_trans.json\", 'w', encoding='utf-8') as f:\r\n",
        "#    json.dump(obj, f, ensure_ascii=False)\r\n",
        "\r\n",
        "\r\n",
        "#with open(\"./seokjinmoonNLU/friends_en/\"+\"friends_test_trans.json\", encoding='utf-8') as jsonfile:\r\n",
        "#    obj2 = json.load(jsonfile)\r\n",
        "\r\n",
        "\r\n",
        "train = pd.read_json(\"seokjinmoonNLU/friends_en/\"+\"friends_train.json\", encoding='utf-8-sig')\r\n",
        "dev = pd.read_json(\"seokjinmoonNLU/friends_en/\"+\"friends_dev.json\", encoding='utf-8-sig')\r\n",
        "test = pd.read_json(\"seokjinmoonNLU/friends_en/\"+\"friends_test.json\", encoding='utf-8-sig')\r\n",
        "\r\n",
        "size = train.size\r\n",
        "\r\n",
        "trainDF = pd.DataFrame(index=range(0,size), columns=['emotion', 'utterance'])\r\n",
        "print(trainDF.dtypes)\r\n",
        "\r\n",
        "trainDF = trainDF.fillna(0)\r\n",
        "print(train[0][0]['emotion'])\r\n",
        "#print(train[1][0])\r\n",
        "for i in range(0, 719):\r\n",
        "  for j in range(0, 23):\r\n",
        "    trainDF.iloc[i+j, trainDF.columns.get_loc('emotion')] = train[i][j]['emotion']\r\n",
        "    trainDF.iloc[i+j, trainDF.columns.get_loc('utterance')] = train[i][j]['utterance']\r\n",
        "\r\n",
        "#trainDF.insert(loc=0, column='emotion', value=1)\r\n",
        "\r\n",
        "\r\n",
        "print(trainDF)\r\n",
        "#trainDF\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#temp.columns = ['id', 'document']\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "emotion      object\n",
            "utterance    object\n",
            "dtype: object\n",
            "neutral\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-8bc9d6e7e704>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m719\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m23\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emotion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utterance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utterance'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeH3pNpKE3bC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}